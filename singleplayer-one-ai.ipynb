{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "LOAD_FROM_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, choice\n",
    "\n",
    "from matplotlib import cm\n",
    "from time import sleep\n",
    "from colosseumrl.envs.tron import TronGridEnvironment, TronRender, TronRllibEnvironment\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Dict, Discrete, Box\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DEFAULT_CONFIG\n",
    "\n",
    "from ray.rllib.models.preprocessors import Preprocessor\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 1517\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Agent\n",
    "\n",
    "##### Thinking of a more intelligent agent is pretty hard. So let's make machine learning find one for us! First, let's train an agent to defeat our personal atempt. We will employ Rllib in order to train an agent using Deep Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our manual agent again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAvoidAgent:\n",
    "    \"\"\" Basic single player agent to test single player version of Tron. \"\"\"\n",
    "    def __init__(self, noise=0.1):\n",
    "        self.noise = noise\n",
    "\n",
    "    def __call__(self, env, observation):\n",
    "        # With some probability, select a random action for variation\n",
    "        if random() <= self.noise:\n",
    "            return choice([0, 1, 2])\n",
    "        \n",
    "        # Get game information\n",
    "        board = observation['board']\n",
    "        head = observation['heads'][0]\n",
    "        direction = observation['directions'][0]\n",
    "        \n",
    "        # Find the head of our body\n",
    "        board_size = board.shape[0]\n",
    "        x, y = head % board_size, head // board_size\n",
    "\n",
    "        # Check ahead. If it's clear, then take a step forward.\n",
    "        nx, ny = env.next_cell(x, y, direction, board_size)\n",
    "        if board[ny, nx] == 0:\n",
    "            return 0\n",
    "\n",
    "        # Check a random direction. If it's clear, then go there.\n",
    "        offset, action, backup = choice([(1, 1, 2), (-1, 2, 1)])\n",
    "        nx, ny = env.next_cell(x, y, (direction + offset) % 4, board_size)\n",
    "        if board[ny, nx] == 0:\n",
    "            return action\n",
    "\n",
    "        # Otherwise, go the opposite direction.\n",
    "        return backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Player Tron\n",
    "##### We create a simpler variant of tron featuring only one actively participating agent. This will simplify the RL task to training an agent to play against a fixed set of opponents. We can imagine this as embedding our manual agents within the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePlayer(gym.Env):\n",
    "    \"\"\" Transform tron into a single player game with predefined enemy agents. \"\"\"\n",
    "    def __init__(self, env, active_player = '0', agents = SimpleAvoidAgent()):       \n",
    "        if not isinstance(agents, list):\n",
    "            agents = [agents]\n",
    "        \n",
    "        self.agents = agents\n",
    "        self.active_player = active_player\n",
    "        self.env = env\n",
    "        \n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "        self.observations = None\n",
    "        \n",
    "        self.weiner = 5\n",
    "        \n",
    "    def reset(self):\n",
    "        self.observations = self.env.reset()\n",
    "        return self.observations[self.active_player]\n",
    "        \n",
    "    def step(self, action, agents = None):\n",
    "        if agents is None:\n",
    "            agents = self.agents\n",
    "        \n",
    "        num_agents = len(agents)\n",
    "        actions = {}\n",
    "        \n",
    "        agent_id = 0\n",
    "        for player in self.env.players:\n",
    "            player = str(player)\n",
    "            \n",
    "            if player == self.active_player:\n",
    "                actions[player] = action\n",
    "            else:\n",
    "                actions[player] = agents[agent_id](self.env.env, self.observations[player])\n",
    "                agent_id  = (agent_id + 1) % num_agents\n",
    "        \n",
    "        self.observations, rewards, dones, info = self.env.step(actions)\n",
    "        \n",
    "#         for i,j in rewards.items():\n",
    "#             if j == 10:\n",
    "#                 #print(rewards)\n",
    "#                 #print(\"Player \" + str(i) + \" Won\")\n",
    "#                 self.weiner = i\n",
    "                #print(i)\n",
    "        \n",
    "        return self.observations[self.active_player], rewards[self.active_player], dones[self.active_player], info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Preprocessing\n",
    "##### Often times the original form of the observation is not ideal for neural network input. Therefore, we have to pre-process the observation to extract the key bits of information so that the network can easily learn a value or policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TronExtractBoard(Preprocessor):\n",
    "    \"\"\" Wrapper to extract just the board from the game state and simplify it for the network. \"\"\"        \n",
    "    def _init_shape(self, obs_space, options):\n",
    "        board_size = env.observation_space['board'].shape[0]\n",
    "        return (board_size + 2, board_size + 2, 2)\n",
    "    \n",
    "    def transform(self, observation):\n",
    "        if 'board' in observation:\n",
    "            return self._transform(observation)\n",
    "        else:\n",
    "            return {player: self._transform(obs, int(player)) for player, obs in observation.items()}\n",
    "\n",
    "    def _transform(self, observation, rotate: int = 0):\n",
    "        board = observation['board'].copy()\n",
    "        \n",
    "        # Make all enemies look the same\n",
    "        board[board > 1] = -1\n",
    "        \n",
    "        # Mark where all of the player heads are\n",
    "        heads = np.zeros_like(board)\n",
    "        \n",
    "        if (rotate != 0):\n",
    "            heads.ravel()[observation['heads']] += 1 + ((observation['directions'] - rotate) % 4)\n",
    "            \n",
    "            board = np.rot90(board, k=rotate)\n",
    "            heads = np.rot90(heads, k=rotate)\n",
    "            \n",
    "        else:\n",
    "            heads.ravel()[observation['heads']] += 1 + observation['directions']\n",
    "            \n",
    "        # Pad the outsides so that we know where the wall is\n",
    "        board = np.pad(board, 1, 'constant', constant_values=-1)\n",
    "        heads = np.pad(heads, 1, 'constant', constant_values=-1)\n",
    "        \n",
    "        # Combine together\n",
    "        board = np.expand_dims(board, -1)\n",
    "        heads = np.expand_dims(heads, -1)\n",
    "        \n",
    "        return np.concatenate([board, heads], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(render, env, trainer, frame_time = 0.1):\n",
    "    policy = trainer.get_policy()\n",
    "    policy.cur_epsilon_value = 0\n",
    "    render.close()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    action = None\n",
    "    reward = None\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = trainer.compute_action(state, prev_action=action, prev_reward=reward)\n",
    "\n",
    "        state, reward, done, results = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "        render.render(env.env.state)\n",
    "\n",
    "        sleep(frame_time)\n",
    "    \n",
    "        #print(env.env.players)\n",
    "    #print(env.env.state)\n",
    "    render.render(env.env.state)\n",
    "    \n",
    "    print(reward)\n",
    "    if reward == 10:\n",
    "        return True \n",
    "#     if 1 in env.env.state[-1]:\n",
    "#         print(\"AI Lost\")\n",
    "#     else:\n",
    "#         print(\"AI Won\")\n",
    "\n",
    "    #print(rewards)\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 20:26:59,307\tINFO resource_spec.py:212 -- Starting Ray with 4.79 GiB memory available for workers and up to 2.42 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-11 20:26:59,571\tWARNING services.py:1080 -- Failed to start the dashboard. The dashboard requires Python 3 as well as 'pip install aiohttp psutil setproctitle grpcio'.\n",
      "2020-03-11 20:26:59,816\tINFO trainer.py:377 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-03-11 20:26:59,857\tINFO trainer.py:524 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-03-11 20:26:59,922\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "2020-03-11 20:27:02,400\tWARNING util.py:41 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "\u001b[2m\u001b[36m(pid=57995)\u001b[0m 2020-03-11 20:27:06,393\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=57996)\u001b[0m 2020-03-11 20:27:06,392\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=57997)\u001b[0m 2020-03-11 20:27:06,377\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=57998)\u001b[0m 2020-03-11 20:27:06,377\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "2020-03-11 20:27:08,357\tWARNING trainable.py:210 -- Getting current IP.\n",
      "2020-03-11 20:27:08,358\tINFO trainable.py:416 -- Restored on 192.168.24.54 from checkpoint: /Users/MasterKashani/ray_results/DQN_tron_single_player_2020-03-11_20-26-59lw04y8sz/checkpoint_1/checkpoint-1\n",
      "2020-03-11 20:27:08,359\tINFO trainable.py:423 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': 128, '_time_total': 5.814941167831421, '_episodes_total': 15}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "Training iteration: 1\n",
      "Training iteration: 2\n",
      "Training iteration: 3\n",
      "Training iteration: 4\n",
      "Training iteration: 5\n",
      "Training iteration: 6\n",
      "Training iteration: 7\n",
      "Training iteration: 8\n",
      "Training iteration: 9\n",
      "Training iteration: 10\n",
      "Training iteration: 11\n",
      "Training iteration: 12\n",
      "Training iteration: 13\n",
      "Training iteration: 14\n",
      "Training iteration: 15\n",
      "Training iteration: 16\n",
      "Training iteration: 17\n",
      "Training iteration: 18\n",
      "Training iteration: 19\n",
      "Training iteration: 20\n",
      "Training iteration: 21\n",
      "Training iteration: 22\n",
      "Training iteration: 23\n",
      "Training iteration: 24\n",
      "Training iteration: 25\n",
      "Training iteration: 26\n",
      "Training iteration: 27\n",
      "Training iteration: 28\n",
      "Training iteration: 29\n",
      "Training iteration: 30\n",
      "Training iteration: 31\n",
      "Training iteration: 32\n",
      "Training iteration: 33\n",
      "Training iteration: 34\n",
      "Training iteration: 35\n",
      "Training iteration: 36\n",
      "Training iteration: 37\n",
      "Training iteration: 38\n",
      "Training iteration: 39\n",
      "Training iteration: 40\n",
      "Training iteration: 41\n",
      "Training iteration: 42\n",
      "Training iteration: 43\n",
      "Training iteration: 44\n",
      "Training iteration: 45\n",
      "Training iteration: 46\n",
      "Training iteration: 47\n",
      "Training iteration: 48\n",
      "Training iteration: 49\n",
      "Training iteration: 50\n",
      "Training iteration: 51\n",
      "Training iteration: 52\n",
      "Training iteration: 53\n",
      "Training iteration: 54\n",
      "Training iteration: 55\n",
      "Training iteration: 56\n",
      "Training iteration: 57\n",
      "Training iteration: 58\n",
      "Training iteration: 59\n",
      "Training iteration: 60\n",
      "Training iteration: 61\n",
      "Training iteration: 62\n",
      "Training iteration: 63\n",
      "Training iteration: 64\n",
      "Training iteration: 65\n",
      "Training iteration: 66\n",
      "Training iteration: 67\n",
      "Training iteration: 68\n",
      "Training iteration: 69\n",
      "Training iteration: 70\n",
      "Training iteration: 71\n",
      "Training iteration: 72\n",
      "Training iteration: 73\n",
      "Training iteration: 74\n",
      "Training iteration: 75\n",
      "Training iteration: 76\n",
      "Training iteration: 77\n",
      "Training iteration: 78\n",
      "Training iteration: 79\n",
      "Training iteration: 80\n",
      "Training iteration: 81\n",
      "Training iteration: 82\n",
      "Training iteration: 83\n",
      "Training iteration: 84\n",
      "Training iteration: 85\n",
      "Training iteration: 86\n",
      "Training iteration: 87\n",
      "Training iteration: 88\n",
      "Training iteration: 89\n",
      "Training iteration: 90\n",
      "Training iteration: 91\n",
      "Training iteration: 92\n",
      "Training iteration: 93\n",
      "Training iteration: 94\n",
      "Training iteration: 95\n",
      "Training iteration: 96\n",
      "Training iteration: 97\n",
      "Training iteration: 98\n",
      "Training iteration: 99\n",
      "Training iteration: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 20:29:12,448\tWARNING trainable.py:210 -- Getting current IP.\n",
      "2020-03-11 20:29:12,449\tINFO trainable.py:416 -- Restored on 192.168.24.54 from checkpoint: /Users/MasterKashani/ray_results/DQN_tron_single_player_2020-03-11_20-26-59lw04y8sz/checkpoint_101/checkpoint-101\n",
      "2020-03-11 20:29:12,450\tINFO trainable.py:423 -- Current state after restoring: {'_iteration': 101, '_timesteps_total': 42496, '_time_total': 112.77161884307861, '_episodes_total': 5249}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "Training iteration: 101\n",
      "Training iteration: 102\n",
      "Training iteration: 103\n",
      "Training iteration: 104\n",
      "Training iteration: 105\n",
      "Training iteration: 106\n",
      "Training iteration: 107\n",
      "Training iteration: 108\n",
      "Training iteration: 109\n",
      "Training iteration: 110\n",
      "Training iteration: 111\n",
      "Training iteration: 112\n",
      "Training iteration: 113\n",
      "Training iteration: 114\n",
      "Training iteration: 115\n",
      "Training iteration: 116\n",
      "Training iteration: 117\n",
      "Training iteration: 118\n",
      "Training iteration: 119\n",
      "Training iteration: 120\n",
      "Training iteration: 121\n",
      "Training iteration: 122\n",
      "Training iteration: 123\n",
      "Training iteration: 124\n",
      "Training iteration: 125\n",
      "Training iteration: 126\n",
      "Training iteration: 127\n",
      "Training iteration: 128\n",
      "Training iteration: 129\n",
      "Training iteration: 130\n",
      "Training iteration: 131\n",
      "Training iteration: 132\n",
      "Training iteration: 133\n",
      "Training iteration: 134\n",
      "Training iteration: 135\n",
      "Training iteration: 136\n",
      "Training iteration: 137\n",
      "Training iteration: 138\n",
      "Training iteration: 139\n",
      "Training iteration: 140\n",
      "Training iteration: 141\n",
      "Training iteration: 142\n",
      "Training iteration: 143\n",
      "Training iteration: 144\n",
      "Training iteration: 145\n",
      "Training iteration: 146\n",
      "Training iteration: 147\n",
      "Training iteration: 148\n",
      "Training iteration: 149\n",
      "Training iteration: 150\n",
      "Training iteration: 151\n",
      "Training iteration: 152\n",
      "Training iteration: 153\n",
      "Training iteration: 154\n",
      "Training iteration: 155\n",
      "Training iteration: 156\n",
      "Training iteration: 157\n",
      "Training iteration: 158\n",
      "Training iteration: 159\n",
      "Training iteration: 160\n",
      "Training iteration: 161\n",
      "Training iteration: 162\n",
      "Training iteration: 163\n",
      "Training iteration: 164\n",
      "Training iteration: 165\n",
      "Training iteration: 166\n",
      "Training iteration: 167\n",
      "Training iteration: 168\n",
      "Training iteration: 169\n",
      "Training iteration: 170\n",
      "Training iteration: 171\n",
      "Training iteration: 172\n",
      "Training iteration: 173\n",
      "Training iteration: 174\n",
      "Training iteration: 175\n",
      "Training iteration: 176\n",
      "Training iteration: 177\n",
      "Training iteration: 178\n",
      "Training iteration: 179\n",
      "Training iteration: 180\n",
      "Training iteration: 181\n",
      "Training iteration: 182\n",
      "Training iteration: 183\n",
      "Training iteration: 184\n",
      "Training iteration: 185\n",
      "Training iteration: 186\n",
      "Training iteration: 187\n",
      "Training iteration: 188\n",
      "Training iteration: 189\n",
      "Training iteration: 190\n",
      "Training iteration: 191\n",
      "Training iteration: 192\n",
      "Training iteration: 193\n",
      "Training iteration: 194\n",
      "Training iteration: 195\n",
      "Training iteration: 196\n",
      "Training iteration: 197\n",
      "Training iteration: 198\n",
      "Training iteration: 199\n",
      "Training iteration: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 20:31:24,767\tWARNING trainable.py:210 -- Getting current IP.\n",
      "2020-03-11 20:31:24,770\tINFO trainable.py:416 -- Restored on 192.168.24.54 from checkpoint: /Users/MasterKashani/ray_results/DQN_tron_single_player_2020-03-11_20-26-59lw04y8sz/checkpoint_201/checkpoint-201\n",
      "2020-03-11 20:31:24,771\tINFO trainable.py:423 -- Current state after restoring: {'_iteration': 201, '_timesteps_total': 86272, '_time_total': 223.79837131500244, '_episodes_total': 9109}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "Training iteration: 201\n",
      "Training iteration: 202\n",
      "Training iteration: 203\n",
      "Training iteration: 204\n",
      "Training iteration: 205\n",
      "Training iteration: 206\n",
      "Training iteration: 207\n"
     ]
    }
   ],
   "source": [
    "# Initialize training environment\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def environment_creater(params=None):\n",
    "    agent = SimpleAvoidAgent(noise=0.05)\n",
    "    return SinglePlayer(TronRllibEnvironment(board_size=13, num_players=4), agents=agent)\n",
    "\n",
    "env = environment_creater()\n",
    "tune.register_env(\"tron_single_player\", environment_creater)\n",
    "ModelCatalog.register_custom_preprocessor(\"tron_prep\", TronExtractBoard)\n",
    "\n",
    "# Configure Deep Q Learning with reasonable values\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 4\n",
    "config['num_gpus'] = 0\n",
    "config[\"timesteps_per_iteration\"] = 128 #big things\n",
    "config['target_network_update_freq'] = 256\n",
    "config['buffer_size'] = 10_000\n",
    "config['schedule_max_timesteps'] = 100_000\n",
    "config['exploration_fraction'] = 0.9\n",
    "config['compress_observations'] = False\n",
    "config['num_envs_per_worker'] = 1 if LOAD_FROM_CHECKPOINT else 4\n",
    "config['train_batch_size'] = 256 #big things\n",
    "config['n_step'] = 2\n",
    "config['seed'] = SEED\n",
    "\n",
    "\n",
    "\n",
    "# We will use a simple convolution network with 3 layers as our feature extractor\n",
    "config['model']['vf_share_layers'] = True\n",
    "config['model']['conv_filters'] = [(64, 5, 2), (128, 3, 2), (256, 3, 2)]\n",
    "config['model']['fcnet_hiddens'] = [256]\n",
    "config['model']['custom_preprocessor'] = 'tron_prep'\n",
    "\n",
    "# Begin training or evaluation\n",
    "\n",
    "trainer = DQNTrainer(config, \"tron_single_player\")\n",
    "\n",
    "winarray = []\n",
    "\n",
    "avg_reward_array = []\n",
    "\n",
    "if True:\n",
    "    num_epoch = 500\n",
    "    #test_epochs = 1\n",
    "    for epoch in range(num_epoch + 1):\n",
    "        print(\"Training iteration: {}\".format(epoch))\n",
    "        res = trainer.train()\n",
    "        #print(f\", Average reward: {res['episode_reward_mean']})\n",
    "        \n",
    "        avg_reward_array.append(res['episode_reward_mean'])\n",
    "              \n",
    "        if epoch % 100 == 0:\n",
    "            checkpoint = trainer.save()\n",
    "            render = TronRender(13, 4)\n",
    "            np.random.seed(SEED)\n",
    "            trainer.restore(checkpoint)\n",
    "            win = 0\n",
    "            for _ in range(20):\n",
    "                winner = test(render, env, trainer)\n",
    "                #print(winner)\n",
    "                \n",
    "                if winner == True:\n",
    "                    win += 1\n",
    "#                     print(\"win\")\n",
    "#                 else:\n",
    "#                     print(\"lost\")\n",
    "                    #print(\"MADSFOIH;ASDLFJAS;LDFASDFASDFASDFJ;ASDJFASD;J;AFSD\")\n",
    "            winarray.append(win/20)\n",
    "        \n",
    "#         if epoch % test_epochs == 0:\n",
    "#             reward = env.test(trainer)\n",
    "# checkpoint = trainer.save()\n",
    "# render = TronRender(13, 4)\n",
    "# np.random.seed(SEED)\n",
    "# trainer.restore(checkpoint)\n",
    "# for _ in range(10):\n",
    "#     test(render, env, trainer, frame_time = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(winarray)\n",
    "plt.xlabel('epochs per 100')\n",
    "plt.ylabel('win %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_reward_array)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checkpoint = trainer.save()\n",
    "# render = TronRender(13, 4)\n",
    "# np.random.seed(SEED)\n",
    "# trainer.restore(checkpoint)\n",
    "# for _ in range(10):\n",
    "#     test(render, env, trainer, frame_time = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
