{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, choice\n",
    "\n",
    "from matplotlib import cm\n",
    "from time import sleep\n",
    "from colosseumrl.envs.tron import TronGridEnvironment, TronRender, TronRllibEnvironment\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Dict, Discrete, Box\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DEFAULT_CONFIG\n",
    "\n",
    "from ray.rllib.models.preprocessors import Preprocessor\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "SEED = 1517\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on better agents\n",
    "#### Now that we have mastered playing against our hand crafted agents, how do we go beyond to achieve some sort of optimum?\n",
    "\n",
    "We use a common technique in reinforcement learning known as self-play. Here, we allow the opponents to update along side us, but with a delay. Once we begin defeating our current opponents a certain percentage of the time, we update their values with our own. This will encourage the policy to continually improve because it has to defeat its previous iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more advanced pre-processor\n",
    "For self-play to work, we need to make sure that the opponents see the exact same configuration of the board that player 0 sees. Otherwise the policies will be very confused and try to make player 0 win even when they're supposed to be opponents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TronExtractBoard(Preprocessor):\n",
    "    \"\"\" Wrapper to extract just the board from the game state and simplify it for the network. \"\"\"        \n",
    "    def _init_shape(self, obs_space, options):\n",
    "        board_size = env.observation_space['board'].shape[0]\n",
    "        return (board_size + 2, board_size + 2, 2)\n",
    "    \n",
    "    def transform(self, observation):\n",
    "        # Pretty hacky way to get the current player number\n",
    "        # Requires having exactly 4 players\n",
    "        board = observation['board']\n",
    "        hor_offset = board.shape[0] // 2 + 2\n",
    "        top_player = board[1, hor_offset]\n",
    "        player_number = {1: 0, 4: 1, 3: 2, 2: 3}[top_player]\n",
    "\n",
    "        return self._transform(observation, player_number)\n",
    "\n",
    "    def _transform(self, observation, rotate: int = 0):\n",
    "        board = observation['board'].copy()\n",
    "        \n",
    "        # Make all enemies look the same\n",
    "        board[board > 1] = -1\n",
    "        \n",
    "        # Mark where all of the player heads are\n",
    "        heads = np.zeros_like(board)\n",
    "        \n",
    "        if (rotate != 0):\n",
    "            heads.ravel()[observation['heads']] += 1 + ((observation['directions'] - rotate) % 4)\n",
    "            \n",
    "            board = np.rot90(board, k=rotate)\n",
    "            heads = np.rot90(heads, k=rotate)\n",
    "            \n",
    "        else:\n",
    "            heads.ravel()[observation['heads']] += 1 + observation['directions']\n",
    "            \n",
    "        # Pad the outsides so that we know where the wall is\n",
    "        board = np.pad(board, 1, 'constant', constant_values=-1)\n",
    "        heads = np.pad(heads, 1, 'constant', constant_values=-1)\n",
    "        \n",
    "        # Combine together\n",
    "        board = np.expand_dims(board, -1)\n",
    "        heads = np.expand_dims(heads, -1)\n",
    "        \n",
    "        return np.concatenate([board, heads], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamTron(TronRllibEnvironment):\n",
    "    def step(self, action_dict):\n",
    "        observation, reward_dict, done_dict, info_dict = super().step(action_dict)\n",
    "        return observation, reward_dict, done_dict, info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(render, env, trainer, frame_time = 0.1):\n",
    "#     policy = trainer.get_policy(\"training_policy\")\n",
    "#     policy.cur_epsilon_value = 0\n",
    "#     render.close()\n",
    "#     observation = env.reset()\n",
    "#     done = False\n",
    "#     action = None\n",
    "#     reward = None\n",
    "#     cumulative_reward = 0\n",
    "\n",
    "#     while not done:\n",
    "#         action = trainer.compute_action(observation, prev_action=action, prev_reward=reward, policy_id='training_policy')\n",
    "\n",
    "#         observation, reward, done, results = env.step(action)\n",
    "#         cumulative_reward += reward\n",
    "#         render.render(env.env.state)\n",
    "\n",
    "#         sleep(frame_time)\n",
    "#     print(state)\n",
    "#     render.render(env.env.state)\n",
    "#     return cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A function that updates the opponent policy with the current training policy weights\n",
    "def synchronize_policies(trainer):\n",
    "    training_policy = trainer.get_policy(\"training_policy\")\n",
    "    opponent_policy = trainer.get_policy(\"opponent_policy\")\n",
    "    opponent_policy.set_weights(training_policy.get_weights())\n",
    "    \n",
    "# A callback to caclulate the win percentage after each episode\n",
    "# We will use this to determine when to update the opponenets\n",
    "def on_episode_end(info):\n",
    "    episode = info[\"episode\"]\n",
    "    reward_history = episode._agent_reward_history[\"0\"]\n",
    "    reward = 0\n",
    "    if len(reward_history) > 1:\n",
    "        reward = (reward_history[-1] + 1) / 11\n",
    "        \n",
    "    episode.custom_metrics['final_reward'] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-09 18:02:10,481\tINFO resource_spec.py:212 -- Starting Ray with 4.0 GiB memory available for workers and up to 2.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-09 18:02:10,749\tWARNING services.py:1080 -- Failed to start the dashboard. The dashboard requires Python 3 as well as 'pip install aiohttp psutil setproctitle grpcio'.\n",
      "2020-03-09 18:02:11,005\tINFO trainer.py:377 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-03-09 18:02:11,052\tINFO trainer.py:524 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-03-09 18:02:11,102\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "2020-03-09 18:02:13,923\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "2020-03-09 18:02:16,415\tWARNING util.py:41 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\u001b[2m\u001b[36m(pid=39277)\u001b[0m 2020-03-09 18:02:21,025\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=39276)\u001b[0m 2020-03-09 18:02:21,012\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=39279)\u001b[0m 2020-03-09 18:02:21,016\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=39275)\u001b[0m 2020-03-09 18:02:21,048\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=39277)\u001b[0m 2020-03-09 18:02:22,883\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=39276)\u001b[0m 2020-03-09 18:02:22,889\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=39279)\u001b[0m 2020-03-09 18:02:22,867\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n",
      "\u001b[2m\u001b[36m(pid=39275)\u001b[0m 2020-03-09 18:02:22,907\tWARNING catalog.py:390 -- DeprecationWarning: Custom preprocessors are deprecated, since they sometimes conflict with the built-in preprocessors for handling complex observation spaces. Please use wrapper classes around your environment instead of preprocessors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Average reward: 13.666666666666666\n",
      "Training iteration: 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Average reward: 11.590909090909092\n"
     ]
    }
   ],
   "source": [
    "# Initialize training environment\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "# def environment_creater(params=None):\n",
    "#     return TronRllibEnvironment(board_size=21, num_players=4)\n",
    "    \n",
    "def team_environment_creater(params=None):\n",
    "    return TeamTron(board_size=21, num_players=4)   \n",
    "    \n",
    "env = team_environment_creater()\n",
    "\n",
    "#tune.register_env(\"tron_multi_player\", environment_creater)\n",
    "tune.register_env(\"tron_team\", team_environment_creater)\n",
    "ModelCatalog.register_custom_preprocessor(\"tron_prep\", TronExtractBoard)\n",
    "\n",
    "# Configure Deep Q Learning for multi-agent training\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 4\n",
    "config[\"timesteps_per_iteration\"] = 128\n",
    "config['target_network_update_freq'] = 256\n",
    "config['buffer_size'] = 10_000\n",
    "config['schedule_max_timesteps'] = 100_000\n",
    "config['exploration_fraction'] = 0.9\n",
    "config['compress_observations'] = False\n",
    "config['num_envs_per_worker'] = 1\n",
    "config['train_batch_size'] = 256\n",
    "config['n_step'] = 2\n",
    "config['callbacks'] = { \n",
    "        \"on_episode_end\": on_episode_end,\n",
    "    }\n",
    "\n",
    "# All of the models will use the same network as before\n",
    "agent_config = {\n",
    "    \"model\": {\n",
    "        \"vf_share_layers\": True,\n",
    "        \"conv_filters\": [(64, 5, 2), (128, 5, 2), (256, 5, 2)],\n",
    "        \"fcnet_hiddens\": [128],\n",
    "        \"custom_preprocessor\": 'tron_prep'\n",
    "    }\n",
    "}\n",
    "\n",
    "config['multiagent'] = {\n",
    "        \"policies_to_train\": [\"training_policy\"],\n",
    "        \"policy_mapping_fn\": lambda x: \"training_policy\" if x == \"0\" else \"opponent_policy\",\n",
    "        \"policies\": {\"training_policy\": (None, env.observation_space, env.action_space, agent_config),\n",
    "                     \"opponent_policy\": (None, env.observation_space, env.action_space, agent_config)}\n",
    "}\n",
    "       \n",
    "trainer = DQNTrainer(config, \"tron_team\")\n",
    "num_epoch = 2\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    print(\"Training iteration: {}\".format(epoch), end='')\n",
    "    res = trainer.train()\n",
    "    print(f\", Average reward: {res['policy_reward_mean']['training_policy']}\")\n",
    "    \n",
    "    if res['custom_metrics']['final_reward_mean'] > 0.6:\n",
    "        print(\"Updating opponents\")\n",
    "        synchronize_policies(trainer)\n",
    "\n",
    "checkpoint = trainer.save()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def determine_winner(rewardDict):\n",
    "    for i,j in rewardDict.items():\n",
    "        if rewardDict[i] == 10:\n",
    "            return i\n",
    "    return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(render, env, trainer, frame_time = 0.1):\n",
    "    extractBoard = TronExtractBoard(env.observation_space)\n",
    "    policy = trainer.get_policy(\"training_policy\")\n",
    "    policy.cur_epsilon_value = 0\n",
    "    render.close()\n",
    "    obsDict = env.reset()\n",
    "    doneDict = {'__all__':False}\n",
    "    actionDict = {}\n",
    "    rewardDict = {}\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    while not doneDict['__all__']:\n",
    "        for player, obs in obsDict.items():\n",
    "            actionDict[player] = trainer.compute_action(obs, prev_action=actionDict.get(player, None), prev_reward=rewardDict.get(player, None), policy_id='training_policy')\n",
    "\n",
    "        obsDict, rewardDict, doneDict, results = env.step(actionDict)\n",
    "#         cumulative_reward += reward\n",
    "        render.render(env.state)\n",
    "\n",
    "        sleep(frame_time)\n",
    "    \n",
    "    #print(doneDict)\n",
    "    print(\"winner: \", determine_winner(rewardDict))\n",
    "    \n",
    "    render.render(env.state)\n",
    "    #render.close()\n",
    "    return cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-09 18:02:28,342\tWARNING trainable.py:210 -- Getting current IP.\n",
      "2020-03-09 18:02:28,344\tINFO trainable.py:416 -- Restored on 192.168.24.54 from checkpoint: /Users/MasterKashani/ray_results/DQN_tron_team_2020-03-09_18-02-117laqf1s6/checkpoint_2/checkpoint-2\n",
      "2020-03-09 18:02:28,345\tINFO trainable.py:423 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': 288, '_time_total': 11.431609153747559, '_episodes_total': 22}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': True, '1': True, '2': array([2]), '3': True, '__all__': True}\n",
      "winner:  2\n",
      "{'0': True, '1': True, '2': True, '3': array([3]), '__all__': True}\n",
      "winner:  3\n",
      "{'0': True, '1': True, '2': True, '3': array([3]), '__all__': True}\n",
      "winner:  3\n",
      "{'0': True, '1': array([1]), '2': True, '3': True, '__all__': True}\n",
      "winner:  1\n",
      "{'0': True, '1': True, '2': True, '3': array([3]), '__all__': True}\n",
      "winner:  3\n"
     ]
    }
   ],
   "source": [
    "render = TronRender(21, 4)\n",
    "np.random.seed(SEED)\n",
    "trainer.restore(checkpoint)\n",
    "for _ in range(5):\n",
    "    test(render, env, trainer, frame_time=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
